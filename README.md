# Power law attention biases for molecular transformers

Transformers are a flexible, scalable class of architectures which dominate most other domains of deep learning. Can better structural biases help them become competitive with state-of-the-art models in molecular property prediction? We propose and evaluate power law attention biases, which modulate attention probabilities according to a power law of the interatomic distance. We also explore how attention biases in general can compensate for the ablation of scaled dot-product attention. 

This work will appear at the Machine Learning for Physical Sciences Workshop at NeurIPS 2025. Check out our paper [here](https://arxiv.org/abs/2511.11489). 
