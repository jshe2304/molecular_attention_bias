{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f59dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/jshe/molecular_attention_bias/src\")\n",
    "import toml\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import models\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8642ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = '/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U'\n",
    "model_type = 'FixedAttentionTransformer'\n",
    "structural_bias_type = 'PowerLaw'\n",
    "architecture = 'E128H8D8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98be53c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151748381\n",
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151748381/model.pt\n",
      "1321411\n",
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151832234\n",
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151832234/model.pt\n",
      "1321411\n",
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151811012\n",
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151811012/model.pt\n",
      "1321411\n",
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151831103\n",
      "/scratch/midway3/jshe/molecular-attention-bias/homo_lumo_U/FixedAttentionTransformer/PowerLaw/E128H8D8/202508151831103/model.pt\n",
      "1321411\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(log_dir):\n",
    "    if not files or 'model.pt' not in files: continue\n",
    "    *_, this_model_type, this_structural_bias_type, this_architecture, run_id = root.split('/')\n",
    "    if this_model_type != model_type: continue\n",
    "    if this_structural_bias_type != structural_bias_type: continue\n",
    "    if this_architecture != architecture: continue\n",
    "\n",
    "    print(root)\n",
    "\n",
    "    model = models.get_model(\n",
    "        model_type=model_type, \n",
    "        n_tokens=6, \n",
    "        out_features=3, \n",
    "        E=128, H=8, D=8, \n",
    "        dropout=0.1, \n",
    "        #graph_attention_operator_type='MaskedSDPA'\n",
    "        radial_function_type='PowerLaw'\n",
    "    )\n",
    "\n",
    "    model_file = os.path.join(root, 'model.pt')\n",
    "    print(model_file)\n",
    "    state_dict = torch.load(model_file, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbca7668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphAttentionTransformer(\n",
       "  (embed): Embedding(6, 128, padding_idx=0)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-7): 8 x GraphAttentionTransformerBlock(\n",
       "      (attn): MaskedSDPA(\n",
       "        (QKV): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (out_map): Linear(in_features=128, out_features=128, bias=False)\n",
       "      )\n",
       "      (norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_map): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef81f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moltorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
