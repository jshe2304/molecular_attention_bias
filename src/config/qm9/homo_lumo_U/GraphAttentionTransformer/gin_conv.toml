output_dir = "/scratch/midway3/jshe/molecular-attention-bias/"

[model_config]
model_type = "GraphAttentionTransformer"
n_tokens = 6
E = 64
H = 8
D = 8
dropout = 0.1
graph_attention_operator_type = "GINConv"


[train_config]
epochs = 128
batch_size = 64
lr = 0.0001
weight_decay = 0.00001
warmup_epochs = 2
warmup_start_factor = 0.01

[dataset_config]
dataset_type = "MolecularGraphDataset"
data_dir = "/scratch/midway3/jshe/data/qm9/scaffolded/"
target_labels = ['homo', 'lumo', 'U']
