output_dir = "/scratch/midway3/jshe/molecular-attention-bias/qm9/"

[model_config]
model_type = "BiasedAttentionTransformer"
n_tokens = 6
E = 128
H = 8
D = 8
dropout = 0.1
radial_function_type = ["ExpNegativePowerLaw", "Zeros", "Zeros", "Zeros", "Zeros", "Zeros", "Zeros", "Zeros"]
identifier = "Mixed-1"

[train_config]
epochs = 80
batch_size = 64
lr = 0.0001
weight_decay = 0.00001
warmup_epochs = 1
warmup_start_factor = 0.01

[dataset_config]
dataset_type = "PointCloudDataset"
data_dir = "/scratch/midway3/jshe/data/qm9/"
target_labels = ['homo', 'lumo']